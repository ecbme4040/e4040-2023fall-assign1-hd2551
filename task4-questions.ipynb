{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "What is the effect of increasing the number of layers in an MLP? Based on your experiments in task 2/3, are larger models (with a greater number of hidden layers) preferred to smaller ones? Why or why not?\n",
    "\n",
    "   Your answer:\n",
    "   \n",
    "   Increasing the number of layers in an MLP enhances its capacity to learn complex functions and capture intricate data patterns. However, this added complexity comes with challenges. While deeper networks can potentially improve performance on complex tasks, they are more prone to overfitting, especially with limited data. This means they might perform exceptionally well on training data but poorly on unseen data. Additionally, deeper MLPs can face the vanishing or exploding gradient problems during backpropagation, making training more challenging. They also demand more computational resources and time. Training deeper networks requires advanced techniques, such as specific weight initialization and sophisticated optimizers. \n",
    "\n",
    "Regarding the experiments in tasks 2 and 3, the choice between larger and smaller models depends on the dataset's complexity, size, and available computational power. Larger models are favored for intricate tasks with ample data, capturing more detailed features. Conversely, smaller models are apt for simpler tasks or when efficiency is crucial, as they're quicker to train and less likely to overfit. The key is balancing the model's capacity with the task's demands, ensuring optimal performance without overcomplicating the architecture.\n",
    "\n",
    "Between both tasks 2&3, for the dataset provided, the accuracy did not defer significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "What is the significance of activation functions in deep learning models? Name two activation functions that you can use for a hidden layer **and** two for the output layer. \n",
    "\n",
    "   Your answer: \n",
    "\n",
    "Activation functions are pivotal in deep learning, introducing non-linearity to models, enabling them to learn complex patterns beyond the capabilities of linear models. They are instrumental during backpropagation, with their derivatives aiding in the optimization of weights and biases. Furthermore, they determine the activation threshold of neurons, deciding if a neuron should transmit information. This introduction of non-linearity enhances the model's ability to represent intricate, high-dimensional data, making it apt for tasks like image and speech recognition.\n",
    "   \n",
    "   Activation Functions for Hidden Layers:\n",
    "   ReLU (Rectified Linear Unit), \n",
    "   Sigmoid\n",
    "   \n",
    "   Activation funtion for Output Layers:\n",
    "   Softmax, \n",
    "   Tanh (Hyperbolic Tangent), Linear (or Identity) activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Assume you have a problem to predict the annual rainfall in a certain region with some historic numeric data that was given to you, which of these 2 models (Linear Regression vs. Logistic Regression) would you use? How would you modify the problem statement to use the other model? Do they both adopt a linear decison boundary? \n",
    "\n",
    "   Your answer: \n",
    "\n",
    "For predicting the annual rainfall in a certain region based on historic numeric data, **Linear Regression** would be the appropriate choice. This is because rainfall prediction is a regression problem where the output is a continuous value (amount of rainfall), and Linear Regression is designed to predict continuous outputs.\n",
    "\n",
    "To modify the problem statement to use **Logistic Regression**, you would need to turn it into a classification problem. For instance, you could predict whether the annual rainfall in a certain region will be \"above average\" or \"below average\" based on historic data. In this case, you'd categorize the rainfall data into two classes and use Logistic Regression to predict the probability of the rainfall being in one of those classes.\n",
    "\n",
    "Regarding the decision boundary:\n",
    "- **Linear Regression** does not have a decision boundary in the traditional sense, as it predicts a continuous output. However, the relationship it captures between the input and output is linear.\n",
    "  \n",
    "- **Logistic Regression**, on the other hand, does have a decision boundary. It predicts the probability that a given input point belongs to a particular category. Despite its name, Logistic Regression can capture non-linear relationships with the help of feature engineering, but the decision boundary in its basic form is linear.\n",
    "\n",
    "In summary, while both models are linear in terms of their relationship between input and output, only Logistic Regression has a linear decision boundary for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What will happen if you choose a very small or a very large learning rate? \n",
    "\n",
    "   Your answer:\n",
    "   \n",
    "   The learning rate is vital in model training. A very small learning rate leads to slow convergence as the model takes tiny steps during optimization. While it might achieve a precise solution, there's a risk of the model getting stuck in local minima. On the other hand, a very large learning rate can cause the model to overshoot the optimal point, leading to divergence or increasing loss values. It might show rapid initial progress, but this can be misleading as oscillations around the minimum or missing the optimal point altogether are common. In essence, while a small learning rate ensures careful progress, it might be too slow or get trapped. A large learning rate speeds up training but can be unstable or miss the best solution. Techniques like learning rate annealing or adaptive methods can help adjust the rate for better training outcomes.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the interpretation of **perplexity** in t-SNE? How did you set this value during the tuning in task3?\n",
    "    \n",
    "   Your answer:\n",
    "   \n",
    "   In t-SNE (t-distributed Stochastic Neighbor Embedding), **perplexity** is a hyperparameter that balances attention between local and global aspects of the data. It can be interpreted as a smooth measure of the effective number of neighbors. A lower perplexity emphasizes local structures, while a higher value considers more global relationships.\n",
    "\n",
    "For the code in task 3, the perplexity value was initially set to `25.0` when visualizing the t-SNE of the original data. Later in the tuning section, multiple values of perplexity were explored, specifically `5`, `30`, and `50`, in combination with different learning rates.\n",
    "\n",
    "Regarding the decision on setting the perplexity value during tuning, a loop was implemented to iterate over various perplexities and learning rates to visualize the t-SNE results. This iterative approach helps in identifying the best combination of hyperparameters that provide a meaningful 2D representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
